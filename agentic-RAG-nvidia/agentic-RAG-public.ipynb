{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95e3951-1a61-4926-b53e-5bf65d89cba5",
   "metadata": {},
   "source": [
    "# Agentic RAG HALFRACK - IN LAB SMALL\n",
    "### with local Chroma vector database\n",
    "### Models served from K8s cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8af8c91-1f14-4f64-8d64-4a9781f88066",
   "metadata": {},
   "source": [
    "<img src=\"images/agentic-rag-pipeline.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae712c52-b454-4523-95e9-7cc51df8d9f8",
   "metadata": {},
   "source": [
    "### About this notebook\n",
    "\n",
    "- Single LLM role play in a multi-agent set of tasks\n",
    "- Two data sources are used, RAG and a web search fall back, but more can be added to the query router.  Route A and B are available.  Route C is shown as an example.\n",
    "- NVIDIA NIMS are installed on a K8s cluster and accessed via API calls\n",
    "- Notebook does not need to be run on a GPU enabled machine, all GPU required services are provided by the K8s cluster.\n",
    "- Features code that can assist with clickable source files\n",
    "- Features a method to turn OFF the Agentic processes to show the different in results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ceb37-ee74-45da-b666-f12a2d7ccbb2",
   "metadata": {},
   "source": [
    "### Code credit and inspiration:\n",
    "- https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_self_rag/#llms\n",
    "- https://github.com/NVIDIA/workbench-example-agentic-rag\n",
    "- David O'Dell\n",
    "- Tiffany Fahmy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d304f-dde1-45ed-9c86-eb45de74586b",
   "metadata": {},
   "source": [
    "# Library installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653986bb-82d7-4927-8032-f97771c9a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q gradio==4.44.1\n",
    "# %pip install -q aiofiles===23.2.1\n",
    "# %pip install -q unstructured-client==0.26.0\n",
    "# %pip install -q langchain-nvidia-ai-endpoints==0.2.2\n",
    "# %pip install -q langchain==0.2.16\n",
    "# %pip install -q langchain-community==0.2.17                   \n",
    "# %pip install -q langchain-core==0.2.40\n",
    "# %pip install -q langchain-text-splitters==0.2.4\n",
    "# %pip install -q langchain-openai==0.1.23\n",
    "# %pip install -q pdfminer-six==20231228\n",
    "# %pip install -q pillow-heif==0.18.0\n",
    "# %pip install -q opencv-python==4.10.0.84 \n",
    "# %pip install -q unstructured==0.15.9\n",
    "# %pip install -q unstructured-pytesseract==0.3.12\n",
    "# %pip install -q pi-heif==0.18.0\n",
    "# %pip install -q unstructured-inference==0.7.36\n",
    "# %pip install -q tesseract==0.1.3\n",
    "# %pip install -q pytesseract==0.3.10\n",
    "# %pip install -q langgraph==0.2.15\n",
    "# %pip install -q tiktoken==0.8.0\n",
    "# %pip install -q chromadb==0.5.15\n",
    "# %pip install -q nltk\n",
    "# %pip install -q openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd2cb4-19b9-4f91-b77e-9f64f799a7cf",
   "metadata": {},
   "source": [
    "### Set debug and verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a804c-9ec1-464e-9875-201a30aaa40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "# set_debug(True)\n",
    "# set_verbose(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ac01f3-238c-42da-92be-ed54adb5e591",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ffb6b-813b-47a7-82d9-62001adc3c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7bc58-8680-418e-8279-af8715bd3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import Language model tools\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, NVIDIARerank\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "### import loaders\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain_community.document_loaders.merge import MergedDataLoader\n",
    "\n",
    "### for embedding\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "### status bars and UI and other accessories\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650977f5-c202-4aa7-bde3-212c94c86448",
   "metadata": {},
   "source": [
    "# Declare external services\n",
    "\n",
    "Services that will be hosted outside this application, usually the LLM, the vectordb and anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df82fc3-d5cc-4984-8b82-43114bbb8bb5",
   "metadata": {},
   "source": [
    "## Langsmith Tracing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c177e7-0f84-464f-946b-a1ba74988b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# ### Consider adding these as env vars in AI Workbench to enable LangSmith tracing ###\n",
    "# os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "# os.environ[\"LANGCHAIN_PROJECT\"] = \"agentic-rag\"\n",
    "# os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "# os.environ['LANGCHAIN_API_KEY'] = \"YOUR-KEY\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7154b39-4fdc-4a18-a48f-add9403c9fd2",
   "metadata": {},
   "source": [
    "### Define Local LLM for initial testing\n",
    "\n",
    "##### LLM, Embedding and Rerank NIMs will all have different ports. To find out which node the pods are on:\n",
    "\n",
    "kubectl get pods -n videosearch -o wide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a677f0-b851-4fce-a0cd-a1a9185992cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using NIM LLM service\n",
    "\n",
    "model_id = \"meta/llama-3.1-70b-instruct\"\n",
    "api_url = \"http://YOUR-IP/v1\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=api_url,\n",
    "    api_key=\"mykey1234\",   ### this is a made up key, doesn't actually exist\n",
    "    model=model_id,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c23c53-6a15-4f0c-8cd6-d58f4d6943e0",
   "metadata": {},
   "source": [
    "### Define embeddings options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4142ec38-5c75-41a6-9ba6-ba68ee8bf3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = NVIDIAEmbeddings(\n",
    "    base_url=\"http://YOUR-IP/v1\", \n",
    "    model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "    truncate=\"END\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f39e5bf-56ad-4fee-823e-d57e3b552a34",
   "metadata": {},
   "source": [
    "### Define reranking options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6cefce-2f02-4885-a964-318820e3674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = NVIDIARerank(\n",
    "    base_url=\"http://YOUR-IP/v1\", \n",
    "    model=\"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "    truncate=\"END\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b263cf-798e-49dd-8249-745af64cf7a9",
   "metadata": {},
   "source": [
    "# Vector db Content setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f81069-4f5b-4e40-ba5a-cf27f6c18039",
   "metadata": {},
   "source": [
    "### Load PDF data into loader object\n",
    "\n",
    "##### We want the pdf files to be clickable so we set up a prefix appended to each file that points to the server they are residing at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411985f-9911-491c-9b50-5c89e5b3d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing PDF files\n",
    "pdf_directory = \"docs/pdf\"\n",
    "\n",
    "# Load PDF documents\n",
    "pdf_dir_loader = PyPDFDirectoryLoader(pdf_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cfbb65-a495-442d-88f1-8d0853917096",
   "metadata": {},
   "source": [
    "### Load CSV data into loader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ea245-e5b6-4c4f-9411-4c49f5f961cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data_csv_loader = CSVLoader(\"docs/csv/healthcare.csv\", encoding='windows-1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381133d5-246c-4090-adc5-6ff830cfa474",
   "metadata": {},
   "source": [
    "### view CSV head contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eac0cc-a7c1-4f49-a4fa-601b1de9f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359ac38-62d7-4052-b911-31f6947319ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('docs/csv/healthcare.csv')\n",
    "# print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16048f4e-3fa8-4ffc-a012-ea80772f5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_rows = df.shape[0]\n",
    "# print(f\"Total number of rows: {num_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8bff09-ab2b-4fec-b409-c6a0894536d4",
   "metadata": {},
   "source": [
    "## merge pdf and csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943737aa-fc12-4004-b20a-36a433ad47d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the PDF and CSV loaders into a single dataset\n",
    "merged_loader = MergedDataLoader(loaders=[pdf_dir_loader, patient_data_csv_loader])\n",
    "\n",
    "# Load all the merged documents\n",
    "merged_documents = merged_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc391d19-092d-4b6a-93a7-34027c8c1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(merged_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b7e9c-045b-474e-a72d-cda810298ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CSV file rows are broken down and made into one document per row\n",
    "### 230 PDf file documents, + 50 rows of CSV file = 280 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aecfb9-776e-476f-b020-e98c7ffb50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda77e48-52b5-47a7-9634-13014096f2e3",
   "metadata": {},
   "source": [
    "## Transform source format to include URL for pdf chunks in documents \n",
    "This assumes an nginx instance running and pointing to a mounted pdf directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b4714-1d50-442d-a7c9-0b376764b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepend URL prefix to the source in metadata\n",
    "\n",
    "# pdf_count = 0\n",
    "# csv_count = 0\n",
    "\n",
    "# for doc in merged_documents:\n",
    "#     if 'source' in doc.metadata:\n",
    "#         # Remove the directory part from the source path\n",
    "#         file_name = doc.metadata['source'].replace(pdf_directory + \"/docs\", \"\")\n",
    "#         doc.metadata['source'] = file_name\n",
    "\n",
    "#         # Count the number of PDF and CSV documents\n",
    "#         if file_name.lower().endswith('.pdf'):\n",
    "#             pdf_count += 1\n",
    "#         elif file_name.lower().endswith('.csv'):\n",
    "#             csv_count += 1\n",
    "\n",
    "# # Print the total number of PDF and CSV documents\n",
    "# print(f\"Total PDF documents: {pdf_count}\")\n",
    "# print(f\"Total CSV rows: {csv_count}\")\n",
    "\n",
    "# # Print the updated sources with host URLs to verify\n",
    "# for doc in merged_documents[:5]:  # Print first 5 for verification\n",
    "#     print(doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6fa53c-d634-42a0-aa3a-703af8ed096b",
   "metadata": {},
   "source": [
    "### Chunk and split documents\n",
    "\n",
    "Each document will be chunked and split along the chunk_size parameter.  The overlap parameter will ADD to the amount of characters, so 512 plus 256 overlap will equal a split size of around 800.  An overlap of zero will equal a split size of only the chunk value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb147e-2332-42b3-9ae9-0b873e606651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=512, chunk_overlap=64)\n",
    "doc_splits = text_splitter.split_documents(merged_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579beda8-fbc9-4de5-9c31-3a3e6d5735b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove chroma vector db local db folder from previous run\n",
    "\n",
    "!rm -rf \"./chromadb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be73db8d-f62c-4609-8758-c906a121b188",
   "metadata": {},
   "source": [
    "### Initial embed documents into vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb4b48-437d-4f20-928b-51ce9ad171c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chromadb\",\n",
    ")\n",
    "print('\\n' + 'Time to complete:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7244c-b979-480e-9f24-38a89ddd3a64",
   "metadata": {},
   "source": [
    "### Create direct vectorstore retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa790c-b8df-4539-9e55-340d6973cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d438f1-9de0-4531-825d-c8a008f3119b",
   "metadata": {},
   "source": [
    "# Setup and Test Agent pipeline elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57b2d3-43e5-43f9-a0f7-09533a5ee6e1",
   "metadata": {},
   "source": [
    "### LLM Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07166a-e051-405c-903e-71ad220397f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "llm_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant in a health care clinic. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Chain\n",
    "llm_chain = llm_prompt | llm | StrOutputParser()\n",
    "\n",
    "# test\n",
    "# question = \"Tell me about mental health from a population perspective.\"\n",
    "# generation = llm_chain.invoke({\"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f62f28-004f-42ee-84d9-e975739e587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2a38d-5526-40c2-9618-465a710d0d98",
   "metadata": {},
   "source": [
    "### LLM-only toggle function\n",
    "\n",
    "Send request only to LLM directly, bypass RAG vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84accb68-ce9a-4783-94e3-3983e084175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(question):\n",
    "    generation = llm_chain.invoke({\"question\": question})\n",
    "    \n",
    "    return generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39d22c-a5f1-4f89-b053-48b22483e8c8",
   "metadata": {},
   "source": [
    "### RAG chain setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048e638-e25a-483d-a498-d707645ad342",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://medium.com/@callumjmac/implementing-rag-in-langchain-with-chroma-a-step-by-step-guide-16fc21815339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d531a81-6d4d-405e-975a-01ef1c9679fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant in a health care clinic. \n",
    "    Use the following pieces of retrieved context to answer the question first. Always ensure you directly address the user's question explicitly and focus on providing a clear and accurate answer. \n",
    "    After answering the question, provide actionable next steps as the final part of your response. Ensure the next steps are practical, relevant, and tailored to the context provided. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Keep your responses concise, actionable, and tailored to the context provided. \n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer and Next Steps: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = rag_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# Query vector db and get similarity distance scores\n",
    "\n",
    "# question = \"Tell me about mental health from a population perspective.\"\n",
    "\n",
    "# search_results = vectorstore.similarity_search_with_relevance_scores(question, k=5)\n",
    "\n",
    "# print(results)\n",
    "\n",
    "# documents = [Document(page_content=str(result[0].page_content), metadata={**result[0].metadata, \"score\": result[1]}) for result in search_results]\n",
    "\n",
    "# print(documents)\n",
    "\n",
    "# # # Print the scores and rankings\n",
    "# # for rank, (doc, vector_score) in enumerate(documents, start=1):\n",
    "# #     print(f\"-- --\\n\\nRank: {rank}, Score: {vector_score}, Document: {doc.page_content}\")\n",
    "\n",
    "# # # Generate the response\n",
    "# # generation = rag_chain.invoke({\"context\": [doc.page_content for doc, _ in results], \"question\": question})\n",
    "\n",
    "# for rank, doc in enumerate(documents, start=1):\n",
    "#     print(f\"-- --\\n\\nRank: {rank}, Score: {doc.metadata['score']}, Page: {doc.metadata['page']}, Source: {doc.metadata['source']}, Document: {doc.page_content}\")\n",
    "\n",
    "# # Generate the response\n",
    "# generation = rag_chain.invoke({\"context\": [doc.page_content for doc in documents], \"question\": question})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c83d1-47ad-4ab0-b1d3-61f061d44b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61db24-79d2-4bcf-96a1-ae5f7a9f18f9",
   "metadata": {},
   "source": [
    "### Question Router Chain setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c910c1-738c-4bf7-bf9e-801862b227eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a \n",
    "    user question to a vectorstore or web search. Use the vectorstore exclusively for questions related to patient data, skin cancer, covid, or mental health. \n",
    "    You do not need to be stringent with keywords in the question related to these topics. If **any document** is found to be relevant in the vectorstore, stop immediately and generate the answer using that data. \n",
    "    **Do not perform a web search** if even one relevant document is found, regardless of the overall assessment of other documents.\n",
    "    If **no relevant data** is found at all in the vectorstore, or if the question is unrelated to these topics, use web_search.\n",
    "    \n",
    "    Provide the answer in JSON format with a single key called 'datasource' and a single answer either 'vectorstore' or 'websearch' as the value.\n",
    "    Please do not include a preamble or explanation. Your response should be formatted as follows: \\'{{\"datasource\": \"value\"}}\\'.\n",
    "\n",
    "    Example 1: A question that is not related to patient data, skin cancer, covid, or mental health should return with a response to use the web_search like this: \\'{{\"datasource\": \"websearch\"}}\\'.\n",
    "\n",
    "    Example 2: A question that is related to patient data, skin cancer, covid, or mental health and any relevant data is found in the vectorstore should return a response like this: \\'{{\"datasource\": \"vectorstore\"}}\\'.\n",
    "\n",
    "    Question to route: {question}\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "question_router = router_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# question = \"Tell me about mental health from a population perspective.\"\n",
    "\n",
    "# docs = retriever.invoke(question)\n",
    "\n",
    "# doc_txt = docs[1].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6262974-1b56-4028-b59f-94448e21d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(question_router.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0d21f-907d-4ab7-9238-bce2437bfa9d",
   "metadata": {},
   "source": [
    "### Relevance / Retrieval Grader chain setup\n",
    "\n",
    "Checks index of vectorstore to see if there are relavent docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008df98-8394-49da-8fb8-aefe2c90d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "retrieval_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'relevance_yes_no_score' and no premable or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = retrieval_prompt | llm | JsonOutputParser()\n",
    "\n",
    "# question = \"Tell me about mental health from a population perspective.\"\n",
    "\n",
    "# docs = retriever.invoke(question)\n",
    "\n",
    "# doc_txt = docs[1].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271807e0-6ea2-4a8b-bd60-649d0031bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7446f-1aff-4081-9713-1bc9546f3164",
   "metadata": {},
   "source": [
    "### Hallucination Grader chain setup\n",
    "\n",
    "Checks to see if the generation is grounded in truth using the source documents as a reference.  \n",
    "If the generation is grounded in truth, then the hallucination grader responds positively with Yes.\n",
    "\n",
    "If the generation is NOT grounded in truth and has no relavence with the source documents, the grader responds negatively with No."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261a9a4-de13-4dd8-b082-95305a3e43ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether \n",
    "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score in JSON format with a \n",
    "    single key 'score' and no preamble or explanation, like this \\'{{\\'\"score\": \"yes\"\\'{{\\' or \\'{{\\'\"score\": \"no\"\\'{{\\'. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_grader_prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb218d47-7acd-440e-af60-f9fba993e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d1d7b-a23d-4dac-94e2-76960192372d",
   "metadata": {},
   "source": [
    "### Usefulness Check Answer Grader chain setup\n",
    "\n",
    "Is the answer provided \"useful\" to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f6944-4fee-4971-b3a7-2b81b44ed433",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_grader_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score in JSON format with a \n",
    "    single key 'score' and no preamble or explanation, like this \\'{{\\'\"score\": \"yes\"\\'{{\\' or \\'{{\\'\"score\": \"no\"\\'{{\\'. \n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = answer_grader_prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40221dc2-d2a7-4232-991b-fbc7bf12af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43a82d-9718-4c64-8460-00ba3bd0f59f",
   "metadata": {},
   "source": [
    "### Web Search chain setup\n",
    "\n",
    "uses the python library for Tavily open search.  Create an account and API here:\n",
    "https://blog.tavily.com/getting-started-with-the-tavily-search-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4e751-9e81-4233-8b94-b066dbd838c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = \"YOUR-KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ff2db-eb4e-4d44-904c-ea061abc16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e023ee-4f63-4ade-8b47-aebe394e89f6",
   "metadata": {},
   "source": [
    "# Langgraph Node Functions Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d7af1-0ac3-4433-a12c-c93f99ef0e90",
   "metadata": {},
   "source": [
    "### Graph relations function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a142182-af08-4dea-9245-2922d7c37b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "################################ State ##############################\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add internet search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba0cb4e-20ee-4e4f-9d11-2340e739192d",
   "metadata": {},
   "source": [
    "### Question router function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d75d48-fa85-4d85-849a-b6efddb67761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    print(question)\n",
    "\n",
    "    \n",
    "    # Status message\n",
    "    global router_status, router_choice, routing_agent_panel_content\n",
    "    \n",
    "    target_source = question_router.invoke({\"question\": question})\n",
    "    \n",
    "    print(target_source)\n",
    "\n",
    "    # Initialize colors for routing panel in UI\n",
    "    web_color = \"white\"\n",
    "    rag_color = \"white\"\n",
    "\n",
    "    # Check if source is a dictionary\n",
    "    if isinstance(target_source, dict):\n",
    "        if \"datasource\" in target_source:\n",
    "            print(target_source[\"datasource\"])\n",
    "            router_choice = target_source[\"datasource\"]\n",
    "            \n",
    "            if target_source[\"datasource\"] == \"websearch\":\n",
    "                print(\"---DECISION: ROUTE QUESTION TO WEB SEARCH---\")\n",
    "                router_status = \"success\"\n",
    "                web_color = \"#4CBB17\"\n",
    "                rag_color = \"white\"\n",
    "                \n",
    "            elif target_source[\"datasource\"] == \"vectorstore\":\n",
    "                print(\"---DECISION: ROUTE QUESTION TO RAG---\")\n",
    "                router_status = \"success\"\n",
    "                web_color = \"white\"\n",
    "                rag_color = \"#4CBB17\"\n",
    "                \n",
    "        else:\n",
    "            print(\"Error: 'datasource' key not found in source\")\n",
    "    else:\n",
    "        print(\"Error: source is not a dictionary\")\n",
    "\n",
    "    \n",
    "    # HTML table generation with green-colored cell for the routed choice\n",
    "    # print(rag_color)\n",
    "    # print(web_color)\n",
    "\n",
    "    routing_agent_panel_content = f\"\"\"\n",
    "    <table style=\"width: 100%;\">\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td style=\"padding: 1; width: 50%; background-color: {web_color}; text-align: center;\"><b>Web Search</b></td>\n",
    "                <td style=\"padding: 1; width: 50%; background-color: {rag_color}; text-align: center;\"><b>RAG database</b></td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "\n",
    "    ### finish and return the results\n",
    "    \n",
    "    if router_choice == \"websearch\":\n",
    "        return \"websearch\"\n",
    "    elif router_choice == \"vectorstore\":\n",
    "        return \"vectorstore\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d6044-d9ee-4922-9921-16643bf17be7",
   "metadata": {},
   "source": [
    "### Retrieval function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1aaa5a-8dec-48a7-acb5-cc9da335ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE USING NVIDIA EMBEDDINGS NIM---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "\n",
    "    ### set nearest neighbors here, it will be used in other function as well\n",
    "    \n",
    "    global k_nearest\n",
    "    \n",
    "    k_nearest = 5\n",
    "    \n",
    "    # Retrieval now with vector similarity score\n",
    "    search_results = vectorstore.similarity_search_with_score(question, k_nearest)\n",
    "\n",
    "\n",
    "    documents = [Document(page_content=str(result[0].page_content), metadata={**result[0].metadata, \"score\": result[1]}) for result in search_results]\n",
    "    \n",
    "    # print(documents)\n",
    "    \n",
    "    # Status message\n",
    "    global retrieve_status\n",
    "    \n",
    "    retrieve_status = \"success\"\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842b0c2c-b633-4248-b5b1-53af320f2c18",
   "metadata": {},
   "source": [
    "### Rerank function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56743880-d070-4ce1-9587-d841bce6cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---NVIDIA RERANK NIM PROCESS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "#### reranking NIM will create and reformat the metatdata with a new relevance_score key value, the docs will reflect this new metadata\n",
    "    \n",
    "    # Reranking\n",
    "    documents = reranker.compress_documents(query=question, documents=documents)\n",
    "\n",
    "    # print(documents)\n",
    "\n",
    "    # Status message\n",
    "    global rerank_status\n",
    "    rerank_status = \"success\"\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96ef54-f96a-42c6-b078-9e5255d76603",
   "metadata": {},
   "source": [
    "### Grade document relevance function setup\n",
    "This will grade and create a list of relevant and not relevant docs as well as the count of those docs. This will print to raw output but will also be available for use in other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beee6e9-8712-4c62-900e-d5e1672ead7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "    If no document is relevant, we will set a flag to run web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    \n",
    "    global filtered_docs  # Declare the global variable to place docs in to be accessed in other functions\n",
    "    global not_relevant_docs  # Declare the global variable to place not relevant docs\n",
    "    global not_relevant_count  # Declare the global variable to place not relevant docs\n",
    "    global relevant_count  # Add this line\n",
    "\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    not_relevant_docs = []\n",
    "\n",
    "    web_search = \"Yes\"  # Default to Yes in case there is no relevant doc\n",
    "\n",
    "    ## take the page content value of documents retrieved and grade it against the question,\n",
    "    ## this means the filtered_docs array will only contain page content values, not file names\n",
    "\n",
    "    # Counter for not relevant documents\n",
    "    not_relevant_count = 0\n",
    "    relevant_count = 0  # Initialize relevant_count\n",
    "\n",
    "    \n",
    "### CATEGORIZE RELEVANT VS NOT RELEVANT DOCS\n",
    "\n",
    "\n",
    "    \n",
    "    for doc in documents:\n",
    "        relevance_yes_no_score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": doc.page_content}\n",
    "        )\n",
    "        grade = relevance_yes_no_score[\"relevance_yes_no_score\"]\n",
    "        \n",
    "        print(grade)\n",
    "\n",
    "        # Extract the source from metadata\n",
    "        source = doc.metadata.get('source', 'Unknown Source')\n",
    "        \n",
    "        # Extract a snippet of the page content\n",
    "        snippet = doc.page_content[:200]  # Adjust the number of characters as needed\n",
    "\n",
    "        # # Extract the similarity score - a score of the distance between 2 vectors, lower number is best\n",
    "        # similarity_score = doc.metadata.get('score', 'Unknown Score')\n",
    "\n",
    "        # # Extract the reranker relevance score - the higher the score is best\n",
    "        # rerank_relevance_score = doc.metadata.get('relevance_score', 'Unknown Score')\n",
    "\n",
    "        \n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":   ### set to lower case \n",
    "            # print(f\"---GRADE: DOCUMENT RELEVANT---\\nSource: {source}\\nSnippet: {snippet}\\nVector Distance Score: {similarity_score}\\nRerank Relevance Score: {rerank_relevance_score}\")\n",
    "            filtered_docs.append(doc)\n",
    "            # Since we found at least one relevant document, set web_search to \"No\"\n",
    "            web_search = \"No\"\n",
    "            relevant_count += 1  # Increment the relevant counter\n",
    "            \n",
    "            \n",
    "        # Document not relevant\n",
    "        else:\n",
    "            # print(f\"---GRADE: DOCUMENT NOT RELEVANT---\\nSource: {source}\\nSnippet: {snippet}\\nVector Distance Score: {similarity_score}\\nRerank Relevance Score: {rerank_relevance_score}\")\n",
    "            not_relevant_docs.append(doc)\n",
    "            not_relevant_count += 1  # Increment the counter\n",
    "            # Do not include the document in filtered_docs\n",
    "            # will default to web search = yes\n",
    "            continue\n",
    "\n",
    " \n",
    "    # Status message\n",
    "    global relevance_status\n",
    "    relevance_status = \"success\"\n",
    "\n",
    "    global relevance_report_msg\n",
    "    \n",
    "    # Check if relevant documents are less than half of the total documents\n",
    "    if relevant_count < len(documents) / 2:\n",
    "        next_steps = \"Try rephrasing your question, or adding more documents related to the question.\"\n",
    "    else:\n",
    "        next_steps = \"\"\n",
    "    \n",
    "    relevance_report_msg = f\"\"\"\n",
    "        <p style=\"font-size: large;\">\n",
    "            Found <span style=\"color: black;\">{relevant_count}</span> out of <span style=\"color: black;\">{len(documents)}</span> documents to be most relevant to the question.\n",
    "        </p>\n",
    "        <p style=\"font-size: large\">\n",
    "            {next_steps}\n",
    "        </p>\n",
    "    \"\"\"\n",
    "\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search, \"relevant_count\": relevant_count}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc667f8-c62a-4ec5-98b8-9cd5d2fd4891",
   "metadata": {},
   "source": [
    "### Answer Reliability meter function setup\n",
    "The logic here is that more graded relevant document snippets will typically lead to a more reliable answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8e4ef-68f8-4ec3-9e7c-45d604aebf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_reliability_meter(relevant_count):\n",
    "    global k_nearest\n",
    "\n",
    "    # Increase the number of blocks by a factor of 4 to size the meter and make it look more visible\n",
    "    relevant_count *= 4\n",
    "    # relevant_count = min(relevant_count * 4, 20)  # Limit to 25 blocks max\n",
    "    local_k_nearest = k_nearest * 3\n",
    "\n",
    "    # Calculate relevance percentage based on the scaled values\n",
    "    relevance_percentage = min((relevant_count / local_k_nearest) * 100, 100)\n",
    "    \n",
    "    if relevant_count > local_k_nearest / 2:\n",
    "        # High relevance: more than half of local_k_nearest are relevant\n",
    "        filled_square = \"<span style='color: green;'>&#9632;</span>\" * relevant_count  # Filled square: ■\n",
    "        empty_square = \"<span style='color: lightgray;'>&#9633;</span>\" * (local_k_nearest - relevant_count)  # Empty square: □\n",
    "    else:\n",
    "        # Low relevance: less than or equal to half of local_k_nearest are relevant\n",
    "        filled_square = \"<span style='color: yellow;'>&#9632;</span>\" * relevant_count  # Yellow square: ■\n",
    "        empty_square = \"<span style='color: lightgray;'>&#9633;</span>\" * (local_k_nearest - relevant_count)  # Empty square: □\n",
    "\n",
    "    # return f\"\"\"\n",
    "    # <div style='font-size: 24px; display: flex; align-items: center; justify-content: space-between; \n",
    "    #             width: 100%; max-width: 500px; white-space: nowrap; overflow: hidden;'>\n",
    "    #     <div style='display: flex; gap: 2px; max-width: 420px; overflow: hidden; flex-shrink: 0;'>{filled_square}{empty_square}</div>\n",
    "    #     <span style='font-size: 18px; margin-left: 8px;'>{relevance_percentage:.0f}%</span>\n",
    "    # </div>\"\"\"\n",
    "\n",
    "\n",
    "    # # Return the meter with a percentage display without fixing the width\n",
    "    # return f\"\"\"\n",
    "    # <div style='font-size: 24px; display: flex; flex-wrap: wrap; align-items: center; width: 100%; max-width: 400px;'>\n",
    "    #     {filled_square + empty_square}\n",
    "    #     <span style='font-size: 18px; margin-left: 10px;'>{relevance_percentage:.0f}%</span>\n",
    "    # </div>\"\"\"\n",
    "\n",
    "    return f\"<div style='font-size: 24px; display: flex; align-items: center; justify-content: space-between; width: 100%;'>\" \\\n",
    "           f\"{filled_square + empty_square}\" \\\n",
    "           f\"<span style='font-size: 18px;'>{relevance_percentage:.0f}%</span>\" \\\n",
    "           f\"</div>\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a62dd6-c1e9-4ce8-8876-2b533013c41a",
   "metadata": {},
   "source": [
    "### Decide to Generate or web fallback function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07cd5b-9276-43e2-925c-b28e2263b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    global web_fallback_status\n",
    "    \n",
    "    if web_search == \"Yes\":\n",
    "        # No relevant documents were found, so fall back to web search\n",
    "        print(\n",
    "            \"---DECISION: RAG DOCS DO NOT CONTAIN RELEVANT CONTENT, FALLING BACK TO WEBSEARCH---\"\n",
    "        )\n",
    "        web_fallback_status = \"success\"\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate the answer\n",
    "        print(\"---DECISION: GENERATE ANSWER---\")\n",
    "        return \"generate\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1cb7f2-7752-45f7-b313-4da3e25ebc7d",
   "metadata": {},
   "source": [
    "### Web search function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3dd6ba-1f10-43be-b34c-19616157d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Check if 'documents' key exists in state, if not, initialize it\n",
    "    if \"documents\" not in state:\n",
    "        state[\"documents\"] = []\n",
    "\n",
    "    ## passes existing documents list to function, there may or may not be doc in the array\n",
    "    \n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "    \n",
    "    # Transform the keys from 'url' to 'source' and 'content' to 'page_content'\n",
    "    transformed_docs = [{\"source\": d[\"url\"], \"page_content\": d[\"content\"]} for d in docs]\n",
    "\n",
    "    # Create Document objects with the transformed results\n",
    "    for doc in transformed_docs:\n",
    "        document = Document(page_content=doc['page_content'], metadata={'source': doc['source']})\n",
    "        documents.append(document)\n",
    "\n",
    "\n",
    "    \n",
    "    global relevance_report_msg\n",
    "    \n",
    "    relevance_report_msg = f\"\"\"\n",
    "            <p style=\"font-size: large;\">\n",
    "                RAG database lacked relevant documents, Agent has diverted question to Web Search.\n",
    "            </p>\n",
    "        \"\"\"\n",
    "    \n",
    "    global relevant_count, k_nearest, websearch_status\n",
    "    \n",
    "    relevant_count = len(documents)\n",
    "    k_nearest = len(documents)\n",
    "    websearch_status = \"success\"\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0815a-6a57-4416-8a54-c8e3aa3d54c1",
   "metadata": {},
   "source": [
    "### Generate answer function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988645c-4858-4a65-8d61-1322789bc898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE AN ANSWER---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    \n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ae85e-7eaf-42ab-ace0-bf0896bc7168",
   "metadata": {},
   "source": [
    "### Calculate NVIDIA green rank sources function setup\n",
    "this is for the GUI, the rerank source docs list have a descending shade of green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83bf0c3-8b30-461b-a3e7-46ee03d552e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nvidia_green_intensity(rank, max_rank=1):\n",
    "    \"\"\"\n",
    "    Adjust the green intensity based on rank.\n",
    "    - If there's only 1 document, default to medium green.\n",
    "    - If multiple documents exist, Rank 1 is lightest, and higher ranks get darker.\n",
    "    \"\"\"\n",
    "    nvidia_green_base = (118, 185, 0)  # Base NVIDIA green (#76B900)\n",
    "\n",
    "    # Handle the case where there's only one document (avoid division by zero)\n",
    "    if max_rank == 1:\n",
    "        green_intensity_factor = 0.75  # Default to medium green if only 1 document\n",
    "    else:\n",
    "        normalized_rank = (rank - 1) / (max_rank - 1)  # Rank 1 -> 0 (lightest), max_rank -> 1 (darkest)\n",
    "        green_intensity_factor = 0.5 + (normalized_rank * 0.5)  # Gradual shift to darker green\n",
    "\n",
    "    # Apply the adjusted green intensity\n",
    "    green_intensity = tuple(int(value * green_intensity_factor) for value in nvidia_green_base)\n",
    "\n",
    "    return f'rgb({green_intensity[0]}, {green_intensity[1]}, {green_intensity[2]})'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af5bcf-d5b2-4a5d-adf0-8760fb12eab4",
   "metadata": {},
   "source": [
    "### Usefulness Grade answer vs documents vs question function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa3d08-6a86-4705-a28b-e2721070bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ Conditional Edge ##############################\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---HALLUCINATION CHECKER---\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    ## SOURCE DOCUMENTS HANDLER ##\n",
    "    ## create a new array with source file and page content snippet for use in GUI output\n",
    "    \n",
    "    global filtered_docs_formatted\n",
    "    filtered_docs_formatted = []\n",
    "\n",
    "    for rank, doc in enumerate(documents, start=1):  # Ensure documents is defined\n",
    "        source = doc.metadata['source']\n",
    "        page_content_snippet = doc.page_content[:200]  # Get the first 200 characters of the snippet\n",
    "        color = calculate_nvidia_green_intensity(rank, max_rank=len(documents))  # Dynamic rank-based green\n",
    "\n",
    "        # Append the formatted HTML to the list\n",
    "        filtered_docs_formatted.append(f'''\n",
    "        <table style=\"width: 100%; border-collapse: collapse; background-color: #fff; margin-bottom: 5px;\">\n",
    "            <tr>\n",
    "                <td style=\"width: 5%; background-color: {color}; text-align: center; font-weight: bold; color: white; padding: 5px;\">\n",
    "                    {rank}\n",
    "                </td>\n",
    "                <td style=\"padding: 5px;\">\n",
    "                    <div style=\"font-weight: bold;\">Source:</div>\n",
    "                    <a href=\"{source}\" target=\"_blank\" class=\"custom-link\">{source}</a>\n",
    "                    <br>\n",
    "                    <div style=\"margin-top: 5px;\"><b>Snippet</b>: {page_content_snippet}</div>\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "        ''')\n",
    "\n",
    "\n",
    "    \n",
    "    ### HALLUCINATION CHECK\n",
    "    ### first starts with hallucination grader which compares the answer to the documents\n",
    "    ### a grade of YES means grounded in documents.  \n",
    "    ### a grade of NO would indicate not grounded in docs and would qualify as an hallucination.\n",
    "\n",
    "    ### USEFULNESS CHECK\n",
    "    ### then a usefulness check that compares the answer to the question to ensure it actually answers the question.\n",
    "    ### a grade of YES means the answer addresses the question \n",
    "    ### a grade of NO would indicate the answer fails to address the question.\n",
    "\n",
    "\n",
    "    # Status message\n",
    "    global hallucination_status, usefulness_status, formatted_usefulness_table\n",
    "    \n",
    "    # HTML table template\n",
    "    usefulness_table_template = \"\"\"\n",
    "    <table border=\"1\">\n",
    "      <tr>\n",
    "        <th>Status</th>\n",
    "        <th>Task</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"color: {color1};\">{status1}</td>\n",
    "        <td>Answer is grounded in relevant documents</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"color: {color2};\">{status2}</td>\n",
    "        <td>Answer effectively addresses the question</td>\n",
    "      </tr>\n",
    "    </table>\n",
    "    \"\"\"\n",
    "    \n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "    \n",
    "    # Check whether or not answer is grounded in documents and no hallucinations, yes is pass, no is fail.\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: ANSWER IS GROUNDED IN DOCUMENTS - NO HALLUCINATIONS---\")\n",
    "    \n",
    "        hallucination_status = \"success\"\n",
    "    \n",
    "        # After hallucination check has passed, now check whether the answer addresses the question\n",
    "        print(\"---GRADE ANSWER vs THE QUESTION---\")\n",
    "        \n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        \n",
    "        grade = score[\"score\"]\n",
    "    \n",
    "        # Evaluate the question-answering score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: ANSWER ADDRESSES QUESTION AND IS USEFUL---\")\n",
    "            usefulness_status = \"success\"\n",
    "            \n",
    "            # Update the HTML table\n",
    "            formatted_usefulness_table = usefulness_table_template.format(color1=\"green\", status1=\"&#10004;\", color2=\"green\", status2=\"&#10004;\")\n",
    "            \n",
    "            return \"useful\"\n",
    "            \n",
    "        else:\n",
    "            print(\"---DECISION: ANSWER DOES NOT ADDRESS QUESTION---\")\n",
    "            \n",
    "            # Update the HTML table\n",
    "            formatted_usefulness_table = usefulness_table_template.format(color1=\"green\", status1=\"&#10004;\", color2=\"grey\", status2=\"&#10008;\")\n",
    "            \n",
    "            return \"not useful\"\n",
    "    \n",
    "    # If it's hallucinating, and answer is not related to documents, retry\n",
    "    else:\n",
    "        print(\"---DECISION: POSSIBLE HALLUCINATIONS - ANSWER IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        \n",
    "        # Update the HTML table\n",
    "        formatted_usefulness_table = usefulness_table_template.format(color1=\"grey\", status1=\"&#10008;\", color2=\"grey\", status2=\"&#10008;\")\n",
    "        \n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f21594-00d4-48a8-ae2e-4e55a010b540",
   "metadata": {},
   "source": [
    "# Langgraph Graph Build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ac38de-e31c-4802-8be9-8d55721918ad",
   "metadata": {},
   "source": [
    "### Langgraph node definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f20fc-12b2-4bfd-b551-779689e382c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"rerank\", rerank)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef86f5-f783-4286-ae87-9659369c57e6",
   "metadata": {},
   "source": [
    "### Langgraph build node relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4b9e4-3ba8-47d6-958c-e5a7112ac6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"rerank\")\n",
    "workflow.add_edge(\"rerank\", \"grade_documents\")\n",
    "\n",
    "# workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55cac61-9150-4b37-b0e4-854f4552d103",
   "metadata": {},
   "source": [
    "# Display graph of node and edge logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13951283-27ae-4f0f-932c-b4fe74419df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "# from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "# app = workflow.compile()\n",
    "\n",
    "# display(\n",
    "#     Image(\n",
    "#         app.get_graph().draw_mermaid_png(\n",
    "#             draw_method=MermaidDrawMethod.API\n",
    "#         )\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a48312-b55c-479c-b4ee-d0f4c413b07e",
   "metadata": {},
   "source": [
    "# Agentic Response Function for GUI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05631c2-02c4-4e9c-959d-b9e0338e4f3c",
   "metadata": {},
   "source": [
    "### Status panel formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261f0be-7afe-4b7a-9b35-c3fa88c6d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "import time\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "### used to format and color the status alert\n",
    "\n",
    "def status_update(status):\n",
    "    if status == \"success\":\n",
    "        return '<div style=\"background-color: green; color: white; padding: 5px; border-radius: 5px;\">Completed successfully</div>'\n",
    "    elif status in [\"websearch\", \"vectorstore\"]:\n",
    "        return f'<div style=\"background-color: blue; color: white; padding: 5px; border-radius: 5px;\">{status}</div>'\n",
    "    else:\n",
    "        return '<div style=\"background-color: grey; color: white; padding: 5px; border-radius: 5px;\">Not used</div>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e1d0d-3bd7-469f-9f4c-f4cb52090dee",
   "metadata": {},
   "source": [
    "### MAIN RESPONSE FUNCTION\n",
    "This takes the inputs from the GUI and triggers all the other external functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863fbb02-fd75-40aa-8560-88e8e78adff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agentic_response(question, mode_toggle):\n",
    "\n",
    "   \n",
    "    ### Create and set status globals\n",
    "    ### reset value of previous variable values upon new execution of main function\n",
    "    \n",
    "    global router_status, router_choice, retrieve_status, rerank_status, relevance_status, web_fallback_status, websearch_status, hallucination_status, usefulness_status, filtered_docs_formatted, answer_reliability_content, routing_agent_panel_content, relevance_report_msg, formatted_usefulness_table\n",
    "    \n",
    "    # router_status = None\n",
    "    # router_choice = None\n",
    "    # retrieve_status = None\n",
    "    # rerank_status = None\n",
    "    # relevance_status = None\n",
    "    # web_fallback_status = None\n",
    "    # websearch_status = None\n",
    "    \n",
    "    hallucination_status = None\n",
    "    usefulness_status = None\n",
    "    routing_agent_panel_content = None\n",
    "    relevance_report_msg = None\n",
    "    answer_reliability_content = None\n",
    "    \n",
    "\n",
    "    # print(routing_agent_panel_content)\n",
    "\n",
    "    \n",
    "    # Compile the workflow\n",
    "    app = workflow.compile()\n",
    "\n",
    "    # Prepare the input\n",
    "    inputs = {\"question\": question}\n",
    "\n",
    "    # Initialize the response variable\n",
    "    response = None\n",
    "\n",
    "\n",
    "    # Stream the output from the app\n",
    "\n",
    "    for output in app.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            # Check if 'generation' key is in the value\n",
    "            if 'generation' in value:\n",
    "                response = value['generation']\n",
    "\n",
    "    \n",
    "    graded_response = f\"Response:\\n{response}\"\n",
    "\n",
    "    ### bring in filtered docs formatted from outside function, join the contents to make it look better in textbox in GUI\n",
    "    \n",
    "    filtered_docs_content = \"\\n\\n\".join(filtered_docs_formatted)\n",
    "\n",
    "    \n",
    "    \n",
    "    # bring in Status messages for indicator panel\n",
    "\n",
    "    hallucination_status_result = status_update(hallucination_status)\n",
    "    usefulness_status_result = status_update(usefulness_status)\n",
    "\n",
    "\n",
    "    # if using LLM-toggle turn off the agent status messages since they aren't used\n",
    "    if mode_toggle == \"LLM only mode\":\n",
    "        llm_response = get_llm_response(question)\n",
    "        # router_status_result = \"Not used\"\n",
    "        # router_choice_result = \"Not used\"\n",
    "        # retrieve_status_result = \"Not used\"\n",
    "        # rerank_status_result = \"Not used\"\n",
    "        # relevance_status = \"Not used\"\n",
    "        # web_fallback_status = \"Not used\"\n",
    "        # websearch_status = \"Not used\"\n",
    "        # hallucination_status = \"Not used\"\n",
    "        # usefulness_status = \"Not used\"\n",
    "        # relevance_status_result = \"Not used\"\n",
    "        # web_fallback_status_result = \"Not used\"\n",
    "        # websearch_status_result = \"Not used\"\n",
    "        \n",
    "        hallucination_status_result = \"Not used\"\n",
    "        usefulness_status_result = \"Not used\"\n",
    "        relevance_report_msg = \"LLM only, no relevance check\"\n",
    "        llm_only_source_result = \"LLM only, no source docs\"\n",
    "        routing_agent_panel_content = \"LLM only, no routing\"\n",
    "        answer_reliability_content = \"LLM only, cannot verify answer\"\n",
    "        formatted_usefulness_table = \"LLM only, cannot verify answer\"\n",
    "        \n",
    "        return ( \n",
    "            llm_response, \n",
    "            routing_agent_panel_content, \n",
    "            answer_reliability_content, \n",
    "            relevance_report_msg, \n",
    "            hallucination_status_result, \n",
    "            usefulness_status_result, \n",
    "            formatted_usefulness_table, \n",
    "            llm_only_source_result\n",
    "        )\n",
    "\n",
    "                     \n",
    "\n",
    "    \n",
    "    answer_reliability_content = answer_reliability_meter(relevant_count)\n",
    "        \n",
    "###  the return statement of the function needs to return in the order that matches the outputs in gradio]\n",
    "### in this case we are going from top left to left bottom.  To top right, to right bottom.\n",
    "\n",
    "    return (\n",
    "        response, \n",
    "        routing_agent_panel_content, \n",
    "        answer_reliability_content, \n",
    "        relevance_report_msg, \n",
    "        hallucination_status_result, \n",
    "        usefulness_status_result, \n",
    "        formatted_usefulness_table, \n",
    "        filtered_docs_content\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42229271-2511-4d2a-8b4d-92d941977f74",
   "metadata": {},
   "source": [
    "### Example question array for GUI Example questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f267b1-aa65-4d4d-a964-8cd8343199db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit data below for specific demos ----\n",
    "EXAMPLE_TITLES = [\n",
    "                     \"### Vector Search\",\n",
    "                     \"### Web Fallback\",\n",
    "                     \"### Web Search\",\n",
    "                 ]\n",
    "EXAMPLES = [\n",
    "\n",
    "###Vector Search\n",
    "\n",
    "               [\n",
    "                   \n",
    "        \"Create an email to the head nurse that summarizes the patients admitted for heart related issues.\",\n",
    "        \"What can you say about sunscreen effectiveness in preventing melanoma?\",\n",
    "        \"Please summarize the clinical trial info we have on our drug ipilimumab for melanoma.\",\n",
    "        \"What are the key domains of population-based approaches to mental health?\",\n",
    "\n",
    "               ],\n",
    "\n",
    "### Web Fallback\n",
    "\n",
    "               [\n",
    "        \"What are the FDA-approved treatments for skin cancer in 2024?\",\n",
    "\n",
    "               ],\n",
    "\n",
    "### Web Search\n",
    "\n",
    "\t\t       [ \n",
    "        \"What year did the Bears football team win the super bowl?\",\n",
    "        \"What is the chemical makeup of water?\"\n",
    "               ],\n",
    "\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7728eeb9-9312-488f-a6e3-f68e517ad2af",
   "metadata": {},
   "source": [
    "# GUI setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d7794-c52c-4b56-a66e-f9febf993f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Placeholder URLs for the logos\n",
    "# DELL_LOGO_URL = \"https://upload.wikimedia.org/wikipedia/commons/4/48/Dell_Logo.svg\"\n",
    "# NVIDIA_LOGO_URL = \"https://upload.wikimedia.org/wikipedia/commons/a/a4/NVIDIA_logo.svg\"\n",
    "\n",
    "def clear_fields():\n",
    "    return \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "def check_question(question):\n",
    "    if not question.strip():\n",
    "        gr.Warning(\"No question entered, please input a question.\")\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Default(), title=\"Health Clinic Assistant\") as demo:\n",
    "\n",
    "    # Custom CSS for styling\n",
    "    style = '''\n",
    "    <style>\n",
    "        .custom-html {\n",
    "            border: 1px solid #ccc;\n",
    "            border-radius: 5px;\n",
    "            padding: 8px;\n",
    "            height: 180px;\n",
    "            overflow-y: auto;\n",
    "            background: #fff;\n",
    "        }\n",
    "\n",
    "        .logo-container {\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "        }\n",
    "\n",
    "        .logo-container img {\n",
    "            height: 40px;\n",
    "            width: auto;\n",
    "            margin-right: 15px;\n",
    "        }\n",
    "\n",
    "        body, .gradio-container {\n",
    "            background-color: #f4f4f4;\n",
    "            color: #333;\n",
    "            font-family: Arial, sans-serif;\n",
    "        }\n",
    "\n",
    "        .custom-link {\n",
    "            color: #76B900;\n",
    "            text-decoration: none;\n",
    "            font-weight: bold;\n",
    "        }\n",
    "\n",
    "        # .spaced-column {\n",
    "        # padding-right: 8px; /* Reduce padding so it doesn't push content */\n",
    "        # min-width: 0; /* Prevents forced width constraints */\n",
    "        # flex-grow: 1; /* Allows columns to scale properly */\n",
    "        # max-width: 100%; /* Ensures it doesn’t take extra space */\n",
    "        # }\n",
    "\n",
    "\n",
    "        .spaced-column {\n",
    "            padding-right: 10px; /* Adds space between columns */\n",
    "        }\n",
    "\n",
    "        .status-panel {\n",
    "            border: 2px solid white;\n",
    "            border-radius: 5px;\n",
    "            padding: 8px;\n",
    "            height: 50px;\n",
    "            background-color: white;\n",
    "            display: flex;\n",
    "            align-items: center;\n",
    "            justify-content: center;\n",
    "            width: 100%;\n",
    "            text-align: center;\n",
    "        }\n",
    "    </style>\n",
    "    '''\n",
    "\n",
    "    gr.HTML(style)\n",
    "\n",
    "    # TITLE and LOGOS\n",
    "    with gr.Row():\n",
    "        gr.HTML(f\"\"\"\n",
    "        <div class=\"logo-container\">\n",
    "            <img src=\"/file=images/dell-logo.png\" alt=\"Dell Logo\">\n",
    "            <img src=\"/file=images/nvidia-logo.png\" alt=\"NVIDIA Logo\">\n",
    "        </div>\n",
    "        <h2>Gen AI Health Assistant - Dell Technologies & NVIDIA</h2>\n",
    "        <p>Dataset contains journals on COVID, Skin Cancer and Mental Health as well as simulated clinic patient data</p>\n",
    "        \"\"\")\n",
    "    \n",
    "    # MAIN ROW AFTER TITLE\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "\n",
    "            # QUESTION DROPDOWN\n",
    "            question_dropdown = gr.Dropdown(choices=[question for section in EXAMPLES for question in section],\n",
    "                                            label=\"Select a Question\", \n",
    "                                            interactive=True)\n",
    "\n",
    "    \n",
    "            # MODE\n",
    "            mode_toggle = gr.Dropdown(choices=[\"Agentic RAG mode\", \"LLM only mode\"], \n",
    "                                      label=\"Select Mode\", value=\"Agentic RAG mode\", \n",
    "                                      interactive=True)\n",
    "            \n",
    "            # QUESTION\n",
    "            question = gr.Textbox(label=\"Prompt\", placeholder=\"Enter your question here...\", lines=2, max_lines=2)\n",
    "\n",
    "\n",
    "            # Auto-populate selected question into the textbox\n",
    "            question_dropdown.change(\n",
    "                fn=lambda q: q,\n",
    "                inputs=[question_dropdown],\n",
    "                outputs=[question]\n",
    "            )\n",
    "\n",
    "            # BUTTONS\n",
    "            with gr.Row():  \n",
    "                submit_button = gr.Button(\"Submit\")\n",
    "                clear_button = gr.Button(\"Clear\")\n",
    "                stop_btn = gr.Button(\"Stop Process\")\n",
    "\n",
    "            # RESPONSE\n",
    "            response = gr.Textbox(label=\"Response\", lines=16, max_lines=16)\n",
    "\n",
    "        ################### RIGHT COLUMN\n",
    "        with gr.Column(scale=1):  \n",
    "\n",
    "            # ROW FOR STATUS PANELS (Reliability Meter, Hallucination, Usefulness) - FIXED\n",
    "            with gr.Row(equal_height=True):\n",
    "                with gr.Column(scale=4, min_width=270, elem_classes=\"spaced-column\"):\n",
    "                    gr.Markdown(\"#### Answer Reliability Meter\")\n",
    "                    answer_reliability_content = gr.HTML(\"<div class='status-panel'>Reliability Content</div>\")\n",
    "                \n",
    "                with gr.Column(scale=2, min_width=100, elem_classes=\"spaced-column\"):\n",
    "                    gr.Markdown(\"#### Hallucination Check\")\n",
    "                    hallucination_status_result = gr.HTML(\"<div class='status-panel'>Hallucination Check Result</div>\")\n",
    "                \n",
    "                with gr.Column(scale=2, min_width=100):\n",
    "                    gr.Markdown(\"#### Usefulness Check\")\n",
    "                    usefulness_status_result = gr.HTML(\"<div class='status-panel'>Usefulness Check Result</div>\")\n",
    "\n",
    "\n",
    "            gr.Markdown(\"<b>Routing Agent Panel</b>\")\n",
    "            with gr.Accordion(\"See Details\", open=False):  \n",
    "                routing_agent_panel_content = gr.HTML(\"<div>Routing Agent Content</div>\")\n",
    "                relevance_report_msg = gr.HTML(\"<div>Relevance Report Content</div>\")\n",
    "\n",
    "            gr.Markdown(\"<b>Rerank Agent Sources and Scores</b>\")\n",
    "            with gr.Accordion(\"See Details\", open=False):  \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        filtered_docs_content = gr.HTML(\"<div>Rerank Agent Sources and Scores Content</div>\")\n",
    "\n",
    "            gr.Markdown(\"#### Usefulness Table\")\n",
    "            formatted_usefulness_table = gr.HTML(\"<div>Usefulness Table Content</div>\")\n",
    "\n",
    "    gr.Markdown(\"<hr>\")\n",
    "    gr.Markdown(\"<hr>\")\n",
    "    \n",
    "    warning_popup = gr.HTML(\"<div style='color: red;'>Please input question</div>\", visible=False)\n",
    "\n",
    "    start_event = submit_button.click(\n",
    "        get_agentic_response, \n",
    "        inputs=[question, mode_toggle], \n",
    "        outputs=[response,\n",
    "                 routing_agent_panel_content,\n",
    "                 answer_reliability_content,\n",
    "                 relevance_report_msg,\n",
    "                 hallucination_status_result, \n",
    "                 usefulness_status_result,\n",
    "                 formatted_usefulness_table,\n",
    "                 filtered_docs_content,\n",
    "                ]\n",
    "    )\n",
    "    \n",
    "    submit_button.click(\n",
    "        check_question, \n",
    "        inputs=[question], \n",
    "        outputs=[warning_popup]\n",
    "    )\n",
    "    \n",
    "    clear_button.click(\n",
    "        clear_fields, \n",
    "        inputs=[], \n",
    "        outputs=[response,\n",
    "                 routing_agent_panel_content,\n",
    "                 answer_reliability_content,\n",
    "                 relevance_report_msg,\n",
    "                 hallucination_status_result, \n",
    "                 usefulness_status_result,\n",
    "                 formatted_usefulness_table,\n",
    "                 filtered_docs_content,\n",
    "                 warning_popup\n",
    "                ]\n",
    "    )    \n",
    "    \n",
    "    stop_btn.click(\n",
    "        fn=None, \n",
    "        inputs=None, \n",
    "        outputs=None, \n",
    "        cancels=[start_event]\n",
    "    )\n",
    "\n",
    "demo.queue(max_size=25)\n",
    "\n",
    "\n",
    "\n",
    "demo.launch(share=False, debug=True, server_name=\"192.168.51.11\", server_port=7868, allowed_paths=[\"images/\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "852e94d3-2be0-4a95-9934-0c32c5eedb85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2746a4-80a0-4c1a-9e4a-3449e0bdf1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG Kernel",
   "language": "python",
   "name": "rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
