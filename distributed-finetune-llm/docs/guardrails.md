# Guardrails
In this section, we will delve into the guardrails that are critical to ensure safety, reliability, and controlled access for Large Language Models.

## Table Of Contents
* [Introduction to Guardrails](#introduction)
* [Importance of Guardrails](#importance-of-guardrails)
* [Examples of Guardrails for Generative AI](#examples-of-guardrails-for-generative-ai)
* [Useful Methods for Guardrails](#useful-methods-for-guardrails)
* [Further Reading](#further-reading)

## Introduction

Guardrails are the boundaries that ensure that AI technologies operate ethically and responsibly. As AI tools become more widely used, guardrails act as safety mechanisms to keep AI models in check, preventing them from generating inaccurate, harmful, or offensive content.

## Importance of Guardrails

Guardrails serve several critical functions, including

1. Maintaining transparency: Transparency is a key aspect of responsible AI usage. It also encourages users to trust that they will receive reliable results from models that protect their privacy and rights. Guardrails help maintain transparency by ensuring that AI models’ decision-making processes are understandable and accountable.

2. Avoiding harmful outcomes: Some unwanted outcomes that guardrails attempt to prevent include:

    1. Hallucinations: Due to a lack of a real-world understanding or limitations in training data, AI models can generate factually incorrect or out-of-context statements that reflect biases in data.

    2. Offensive content: To protect users from harmful content, guardrails actively monitor and block content identified as inappropriate, offensive, or biased.

    3. Malicious information: Guardrails block the generation of information that could be used to incite violence, promote illegal activities, or cause harm to other individuals or communities.


## Examples of Guardrails for Generative AI

1. Content filtering: Monitor and block content labeled as inappropriate, offensive, or biased.

2. Plagiarism checking: Ensure that the content generated by the AI model is original and doesn’t infringe on copyright or intellectual property rights.

3. Fact-checking: Verify the accuracy of content.

4. Creative attribution: Properly attributes generated content to avoid plagiarism and maintain transparency.

5. Rate limiting: Restrict the speed at which content is generated to prevent spamming or manipulation of the system.

6. User feedback mechanism: Create channels for users to report errors or inconsistencies in generated content.

7. Controlled generation: Allows users to guide creative output through specific themes, styles, or boundaries.

8. Impact assessment: Evaluate the potential societal impact of generative content.

9. Deepfake detection: Detect and label deepfakes, realistic images, or videos of generated AI to prevent the spread of misinformation or misleading content.



## Useful Methods for Guardrails

Despite the complexity of guardrails today, improving and adapting the technology is becoming increasingly important as users have found ways to circumvent safety measures. Researchers at Carnegie Mellon recently revealed several techniques to jailbreak guardrails used by major organizations, including Google, OpenAI, and Anthropic. For example, by simply adding a long suffix of characters to prompts, they were able to get the models to generate detailed instructions on how to build a bomb.
Here are some useful methods to effectively enforce guardrails:

1. Adversarial Training: This technique uses the original data to create adversarial examples intended to trick the model. The model is then trained on these adversarial inputs alongside desired inputs to enhance the model’s resilience to attempts to exploit its vulnerabilities.

2. Differential Privacy: This is a method that involves adding noise to training data in order to obscure individual data points. Subsequently, this protects the privacy of specific individuals in the dataset.

3. Privacy Enhancing Technologies (PETs):

    1. Data anonymization: Guardrails use techniques like k-anonymity and differential privacy to mask or aggregate identifiable information.

    2. Encryption: Homomorphic encryption allows the system to perform computations on encrypted data without the need for decryption. This protects sensitive data during processing.

    3. Secure multi-party computation: This enables multiple parties to jointly compute a function without revealing the inputs from either party.


## Further Reading

* [Guarding the Future: The Essential Role of Guardrails in AI - Unite.AI](https://www.unite.ai/guarding-the-future-the-essential-role-of-guardrails-in-ai/)
* [Balancing AI Usage: The Importance of AI Guardrails (cmswire.com)](https://www.cmswire.com/customer-experience/the-importance-of-ai-guardrails-for-marketing/)
* [The Importance of PETs and Guardrails in Developing AI Systems for Private Data | by Mark Craddock | Prompt Engineering | Medium](https://medium.com/prompt-engineering/the-importance-of-pets-and-guardrails-in-developing-ai-systems-for-private-data-3ded78928ac4)
* [NeMo Guardrails Keep AI Chatbots on Track | NVIDIA Blogs](https://blogs.nvidia.com/blog/2023/04/25/ai-chatbot-guardrails-nemo/)
* [Researchers Poke Holes in Safety Controls of ChatGPT and Other Chatbots - The New York Times (nytimes.com)](https://www.nytimes.com/2023/07/27/business/ai-chatgpt-safety-research.html)
* [Using MLflow AI Gateway and Llama 2 to Build Generative AI Apps | Databricks Blog](https://www.databricks.com/blog/using-ai-gateway-llama2-rag-apps)
